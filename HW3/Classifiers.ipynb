{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3: PCA/Hyperparameter/CV\n",
    "Data source: http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = arff.loadarff('4year.arff')\n",
    "df = pd.DataFrame(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bankruptcy'] = (df['class']==b'1')\n",
    "df.drop(columns=['class'], inplace=True)\n",
    "df.columns = ['X{0:02d}'.format(k) for k in range(1,65)] + ['bankruptcy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X01</th>\n",
       "      <th>X02</th>\n",
       "      <th>X03</th>\n",
       "      <th>X04</th>\n",
       "      <th>X05</th>\n",
       "      <th>X06</th>\n",
       "      <th>X07</th>\n",
       "      <th>X08</th>\n",
       "      <th>X09</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X24</th>\n",
       "      <th>X25</th>\n",
       "      <th>X26</th>\n",
       "      <th>X27</th>\n",
       "      <th>X28</th>\n",
       "      <th>X29</th>\n",
       "      <th>X30</th>\n",
       "      <th>X31</th>\n",
       "      <th>X32</th>\n",
       "      <th>X33</th>\n",
       "      <th>X34</th>\n",
       "      <th>X35</th>\n",
       "      <th>X36</th>\n",
       "      <th>X37</th>\n",
       "      <th>X38</th>\n",
       "      <th>X39</th>\n",
       "      <th>X40</th>\n",
       "      <th>X41</th>\n",
       "      <th>X42</th>\n",
       "      <th>X43</th>\n",
       "      <th>X44</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>X58</th>\n",
       "      <th>X59</th>\n",
       "      <th>X60</th>\n",
       "      <th>X61</th>\n",
       "      <th>X62</th>\n",
       "      <th>X63</th>\n",
       "      <th>X64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9.771000e+03</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9792.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9.784000e+03</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9634.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9581.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9.151000e+03</td>\n",
       "      <td>9561.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9696.000000</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>5350.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9605.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9.771000e+03</td>\n",
       "      <td>9.771000e+03</td>\n",
       "      <td>9179.000000</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9719.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9773.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9561.000000</td>\n",
       "      <td>9561.000000</td>\n",
       "      <td>9.792000e+03</td>\n",
       "      <td>9771.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9776.000000</td>\n",
       "      <td>9791.000000</td>\n",
       "      <td>9178.000000</td>\n",
       "      <td>9760.000000</td>\n",
       "      <td>9.771000e+03</td>\n",
       "      <td>9749.000000</td>\n",
       "      <td>9561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.043019</td>\n",
       "      <td>0.596404</td>\n",
       "      <td>0.130959</td>\n",
       "      <td>8.136600</td>\n",
       "      <td>6.465164e+01</td>\n",
       "      <td>-0.059273</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>19.884016</td>\n",
       "      <td>1.882296</td>\n",
       "      <td>0.389040</td>\n",
       "      <td>0.075417</td>\n",
       "      <td>0.210989</td>\n",
       "      <td>0.398902</td>\n",
       "      <td>0.059460</td>\n",
       "      <td>3.017681e+03</td>\n",
       "      <td>0.617918</td>\n",
       "      <td>20.976033</td>\n",
       "      <td>0.064580</td>\n",
       "      <td>-0.019081</td>\n",
       "      <td>62.704589</td>\n",
       "      <td>1.218724</td>\n",
       "      <td>0.066203</td>\n",
       "      <td>-0.070364</td>\n",
       "      <td>0.247742</td>\n",
       "      <td>0.222839</td>\n",
       "      <td>0.451115</td>\n",
       "      <td>1.115883e+03</td>\n",
       "      <td>6.725180</td>\n",
       "      <td>3.946479</td>\n",
       "      <td>5.353531</td>\n",
       "      <td>0.041258</td>\n",
       "      <td>341.625124</td>\n",
       "      <td>8.445313</td>\n",
       "      <td>4.979157</td>\n",
       "      <td>0.058091</td>\n",
       "      <td>2.077261</td>\n",
       "      <td>70.659877</td>\n",
       "      <td>0.487190</td>\n",
       "      <td>-1.072578</td>\n",
       "      <td>3.064235</td>\n",
       "      <td>0.968902</td>\n",
       "      <td>-0.371479</td>\n",
       "      <td>7.356944e+02</td>\n",
       "      <td>6.729892e+02</td>\n",
       "      <td>5.458024</td>\n",
       "      <td>7.274189</td>\n",
       "      <td>112.989701</td>\n",
       "      <td>-0.002370</td>\n",
       "      <td>-0.517222</td>\n",
       "      <td>7.085001</td>\n",
       "      <td>0.469319</td>\n",
       "      <td>10.031638</td>\n",
       "      <td>6.114681</td>\n",
       "      <td>7.402928</td>\n",
       "      <td>7.686330e+03</td>\n",
       "      <td>-0.992263</td>\n",
       "      <td>0.035022</td>\n",
       "      <td>1.133287</td>\n",
       "      <td>0.856053</td>\n",
       "      <td>118.156064</td>\n",
       "      <td>25.194430</td>\n",
       "      <td>2.015157e+03</td>\n",
       "      <td>8.660813</td>\n",
       "      <td>35.949619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.359321</td>\n",
       "      <td>4.587122</td>\n",
       "      <td>4.559074</td>\n",
       "      <td>290.647281</td>\n",
       "      <td>1.475939e+04</td>\n",
       "      <td>6.812754</td>\n",
       "      <td>0.533344</td>\n",
       "      <td>698.697015</td>\n",
       "      <td>17.674650</td>\n",
       "      <td>4.590299</td>\n",
       "      <td>0.528232</td>\n",
       "      <td>74.237274</td>\n",
       "      <td>37.974787</td>\n",
       "      <td>0.533344</td>\n",
       "      <td>1.022731e+05</td>\n",
       "      <td>78.494223</td>\n",
       "      <td>698.757245</td>\n",
       "      <td>0.736143</td>\n",
       "      <td>25.583613</td>\n",
       "      <td>377.204157</td>\n",
       "      <td>5.930840</td>\n",
       "      <td>0.504481</td>\n",
       "      <td>23.889882</td>\n",
       "      <td>8.268015</td>\n",
       "      <td>4.852418</td>\n",
       "      <td>74.037751</td>\n",
       "      <td>3.143938e+04</td>\n",
       "      <td>147.963574</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>340.974268</td>\n",
       "      <td>25.585724</td>\n",
       "      <td>6145.604519</td>\n",
       "      <td>69.690183</td>\n",
       "      <td>58.480776</td>\n",
       "      <td>0.483463</td>\n",
       "      <td>17.341615</td>\n",
       "      <td>621.311292</td>\n",
       "      <td>4.578432</td>\n",
       "      <td>77.056762</td>\n",
       "      <td>87.916989</td>\n",
       "      <td>41.191681</td>\n",
       "      <td>14.174896</td>\n",
       "      <td>3.283705e+04</td>\n",
       "      <td>3.281128e+04</td>\n",
       "      <td>186.414617</td>\n",
       "      <td>290.619843</td>\n",
       "      <td>1993.125597</td>\n",
       "      <td>0.525467</td>\n",
       "      <td>15.737098</td>\n",
       "      <td>287.770829</td>\n",
       "      <td>4.554869</td>\n",
       "      <td>897.307846</td>\n",
       "      <td>90.190534</td>\n",
       "      <td>146.013868</td>\n",
       "      <td>7.605261e+04</td>\n",
       "      <td>77.007971</td>\n",
       "      <td>8.945365</td>\n",
       "      <td>8.038201</td>\n",
       "      <td>26.393305</td>\n",
       "      <td>3230.316692</td>\n",
       "      <td>1099.260821</td>\n",
       "      <td>1.171461e+05</td>\n",
       "      <td>60.838202</td>\n",
       "      <td>483.318623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.458000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-445.910000</td>\n",
       "      <td>-0.045319</td>\n",
       "      <td>-3.794600e+05</td>\n",
       "      <td>-486.820000</td>\n",
       "      <td>-12.458000</td>\n",
       "      <td>-1.848200</td>\n",
       "      <td>-0.032371</td>\n",
       "      <td>-445.910000</td>\n",
       "      <td>-12.244000</td>\n",
       "      <td>-6331.800000</td>\n",
       "      <td>-1460.600000</td>\n",
       "      <td>-12.458000</td>\n",
       "      <td>-1.567500e+06</td>\n",
       "      <td>-6331.800000</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>-12.458000</td>\n",
       "      <td>-1578.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.146300</td>\n",
       "      <td>-12.244000</td>\n",
       "      <td>-1578.700000</td>\n",
       "      <td>-314.370000</td>\n",
       "      <td>-466.340000</td>\n",
       "      <td>-6331.800000</td>\n",
       "      <td>-2.590100e+05</td>\n",
       "      <td>-990.020000</td>\n",
       "      <td>-0.440090</td>\n",
       "      <td>-4940.000000</td>\n",
       "      <td>-1495.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-756.500000</td>\n",
       "      <td>-9.043100</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-3.715000</td>\n",
       "      <td>-445.910000</td>\n",
       "      <td>-7522.000000</td>\n",
       "      <td>-8.833300</td>\n",
       "      <td>-1086.800000</td>\n",
       "      <td>-719.800000</td>\n",
       "      <td>-1.158700e+05</td>\n",
       "      <td>-1.158700e+05</td>\n",
       "      <td>-2834.900000</td>\n",
       "      <td>-6.639200</td>\n",
       "      <td>-3.630700</td>\n",
       "      <td>-13.815000</td>\n",
       "      <td>-837.860000</td>\n",
       "      <td>-0.045239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1033.700000</td>\n",
       "      <td>-1033.700000</td>\n",
       "      <td>-7.132200e+05</td>\n",
       "      <td>-7522.100000</td>\n",
       "      <td>-597.420000</td>\n",
       "      <td>-30.892000</td>\n",
       "      <td>-284.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-12.656000</td>\n",
       "      <td>-1.496500e+04</td>\n",
       "      <td>-0.024390</td>\n",
       "      <td>-0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.263145</td>\n",
       "      <td>0.020377</td>\n",
       "      <td>1.047000</td>\n",
       "      <td>-5.121700e+01</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>1.006675</td>\n",
       "      <td>0.294440</td>\n",
       "      <td>0.009457</td>\n",
       "      <td>0.007608</td>\n",
       "      <td>0.021204</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>2.173550e+02</td>\n",
       "      <td>0.061874</td>\n",
       "      <td>1.448900</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>15.244500</td>\n",
       "      <td>0.920125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.057772</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.035097</td>\n",
       "      <td>3.398450</td>\n",
       "      <td>0.082730</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>47.222750</td>\n",
       "      <td>2.702300</td>\n",
       "      <td>0.297400</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>1.030900</td>\n",
       "      <td>1.097050</td>\n",
       "      <td>0.419750</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.051234</td>\n",
       "      <td>0.025029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.890100e+01</td>\n",
       "      <td>3.651950e+01</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>0.613890</td>\n",
       "      <td>15.836000</td>\n",
       "      <td>-0.047398</td>\n",
       "      <td>-0.035386</td>\n",
       "      <td>0.768760</td>\n",
       "      <td>0.185285</td>\n",
       "      <td>0.128978</td>\n",
       "      <td>0.684900</td>\n",
       "      <td>0.946990</td>\n",
       "      <td>2.184000e+01</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.885722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.356325</td>\n",
       "      <td>4.267700</td>\n",
       "      <td>4.323400e+01</td>\n",
       "      <td>2.938800</td>\n",
       "      <td>2.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.041364</td>\n",
       "      <td>0.467740</td>\n",
       "      <td>0.199290</td>\n",
       "      <td>1.591800</td>\n",
       "      <td>-5.557600e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048820</td>\n",
       "      <td>1.088700</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>0.510450</td>\n",
       "      <td>0.062544</td>\n",
       "      <td>0.143690</td>\n",
       "      <td>0.063829</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>9.065550e+02</td>\n",
       "      <td>0.219130</td>\n",
       "      <td>2.134600</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.030967</td>\n",
       "      <td>35.657000</td>\n",
       "      <td>1.045700</td>\n",
       "      <td>0.050118</td>\n",
       "      <td>0.025931</td>\n",
       "      <td>0.149150</td>\n",
       "      <td>0.386750</td>\n",
       "      <td>0.199240</td>\n",
       "      <td>1.005500e+00</td>\n",
       "      <td>0.470770</td>\n",
       "      <td>3.976400</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>80.884500</td>\n",
       "      <td>4.467900</td>\n",
       "      <td>1.975500</td>\n",
       "      <td>0.046855</td>\n",
       "      <td>1.559400</td>\n",
       "      <td>3.110400</td>\n",
       "      <td>0.613270</td>\n",
       "      <td>0.030427</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.089499</td>\n",
       "      <td>0.032384</td>\n",
       "      <td>1.034800e+02</td>\n",
       "      <td>5.786200e+01</td>\n",
       "      <td>0.232560</td>\n",
       "      <td>1.041100</td>\n",
       "      <td>38.482000</td>\n",
       "      <td>0.006990</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>0.336680</td>\n",
       "      <td>0.221380</td>\n",
       "      <td>1.211800</td>\n",
       "      <td>1.378300</td>\n",
       "      <td>9.503300e+02</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>0.098026</td>\n",
       "      <td>0.958305</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>9.482000</td>\n",
       "      <td>6.283550</td>\n",
       "      <td>7.472900e+01</td>\n",
       "      <td>4.848900</td>\n",
       "      <td>4.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.111130</td>\n",
       "      <td>0.689255</td>\n",
       "      <td>0.410670</td>\n",
       "      <td>2.880400</td>\n",
       "      <td>5.573200e+01</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>0.126940</td>\n",
       "      <td>2.691000</td>\n",
       "      <td>1.970225</td>\n",
       "      <td>0.714290</td>\n",
       "      <td>0.140805</td>\n",
       "      <td>0.513820</td>\n",
       "      <td>0.127935</td>\n",
       "      <td>0.126960</td>\n",
       "      <td>2.412500e+03</td>\n",
       "      <td>0.599580</td>\n",
       "      <td>3.781500</td>\n",
       "      <td>0.126960</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>65.667500</td>\n",
       "      <td>1.208925</td>\n",
       "      <td>0.126675</td>\n",
       "      <td>0.071692</td>\n",
       "      <td>0.360600</td>\n",
       "      <td>0.614030</td>\n",
       "      <td>0.542190</td>\n",
       "      <td>5.236100e+00</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>4.498550</td>\n",
       "      <td>0.434085</td>\n",
       "      <td>0.093553</td>\n",
       "      <td>133.202500</td>\n",
       "      <td>7.627100</td>\n",
       "      <td>4.509700</td>\n",
       "      <td>0.126375</td>\n",
       "      <td>2.284250</td>\n",
       "      <td>12.239750</td>\n",
       "      <td>0.777415</td>\n",
       "      <td>0.080855</td>\n",
       "      <td>0.666480</td>\n",
       "      <td>0.215980</td>\n",
       "      <td>0.081628</td>\n",
       "      <td>1.481050e+02</td>\n",
       "      <td>8.510950e+01</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>1.971200</td>\n",
       "      <td>70.936000</td>\n",
       "      <td>0.084418</td>\n",
       "      <td>0.051082</td>\n",
       "      <td>2.268800</td>\n",
       "      <td>0.530775</td>\n",
       "      <td>0.364155</td>\n",
       "      <td>2.274200</td>\n",
       "      <td>2.426300</td>\n",
       "      <td>4.694550e+03</td>\n",
       "      <td>0.117170</td>\n",
       "      <td>0.242680</td>\n",
       "      <td>0.996163</td>\n",
       "      <td>0.211790</td>\n",
       "      <td>19.506000</td>\n",
       "      <td>9.938200</td>\n",
       "      <td>1.233450e+02</td>\n",
       "      <td>8.363800</td>\n",
       "      <td>9.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.482000</td>\n",
       "      <td>446.910000</td>\n",
       "      <td>22.769000</td>\n",
       "      <td>27146.000000</td>\n",
       "      <td>1.034100e+06</td>\n",
       "      <td>322.200000</td>\n",
       "      <td>38.618000</td>\n",
       "      <td>53209.000000</td>\n",
       "      <td>1704.800000</td>\n",
       "      <td>12.602000</td>\n",
       "      <td>38.618000</td>\n",
       "      <td>3340.900000</td>\n",
       "      <td>2707.700000</td>\n",
       "      <td>38.618000</td>\n",
       "      <td>8.085500e+06</td>\n",
       "      <td>4401.300000</td>\n",
       "      <td>53210.000000</td>\n",
       "      <td>50.266000</td>\n",
       "      <td>1082.600000</td>\n",
       "      <td>26606.000000</td>\n",
       "      <td>396.160000</td>\n",
       "      <td>38.618000</td>\n",
       "      <td>879.860000</td>\n",
       "      <td>400.590000</td>\n",
       "      <td>12.602000</td>\n",
       "      <td>3594.600000</td>\n",
       "      <td>2.037300e+06</td>\n",
       "      <td>11864.000000</td>\n",
       "      <td>9.651800</td>\n",
       "      <td>29526.000000</td>\n",
       "      <td>1083.100000</td>\n",
       "      <td>385590.000000</td>\n",
       "      <td>5534.100000</td>\n",
       "      <td>4260.200000</td>\n",
       "      <td>38.618000</td>\n",
       "      <td>1704.800000</td>\n",
       "      <td>24487.000000</td>\n",
       "      <td>12.602000</td>\n",
       "      <td>112.020000</td>\n",
       "      <td>8007.100000</td>\n",
       "      <td>3443.400000</td>\n",
       "      <td>160.110000</td>\n",
       "      <td>3.020000e+06</td>\n",
       "      <td>3.020000e+06</td>\n",
       "      <td>10337.000000</td>\n",
       "      <td>27146.000000</td>\n",
       "      <td>140990.000000</td>\n",
       "      <td>33.535000</td>\n",
       "      <td>107.680000</td>\n",
       "      <td>27146.000000</td>\n",
       "      <td>446.910000</td>\n",
       "      <td>88433.000000</td>\n",
       "      <td>4784.100000</td>\n",
       "      <td>11678.000000</td>\n",
       "      <td>6.123700e+06</td>\n",
       "      <td>112.020000</td>\n",
       "      <td>226.760000</td>\n",
       "      <td>668.750000</td>\n",
       "      <td>1661.000000</td>\n",
       "      <td>251570.000000</td>\n",
       "      <td>108000.000000</td>\n",
       "      <td>1.077900e+07</td>\n",
       "      <td>5662.400000</td>\n",
       "      <td>21153.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X01          X02  ...          X63           X64\n",
       "count  9791.000000  9791.000000  ...  9749.000000   9561.000000\n",
       "mean      0.043019     0.596404  ...     8.660813     35.949619\n",
       "std       0.359321     4.587122  ...    60.838202    483.318623\n",
       "min     -12.458000     0.000000  ...    -0.024390     -0.000015\n",
       "25%       0.001321     0.263145  ...     2.938800      2.012900\n",
       "50%       0.041364     0.467740  ...     4.848900      4.041600\n",
       "75%       0.111130     0.689255  ...     8.363800      9.413500\n",
       "max      20.482000   446.910000  ...  5662.400000  21153.000000\n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.bankruptcy == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean(), inplace=True)\n",
    "df.isna().sum()\n",
    "X_imp = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = X_imp[:, :-1], X_imp[:, -1]\n",
    "y = y.astype(bool)\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6854, 64)\n",
      "(2938, 64)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as skpre\n",
    "\n",
    "stdsc = skpre.StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "print(X_train_std.shape)\n",
    "X_test_std = stdsc.transform(X_test)\n",
    "print(X_test_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline of:\n",
    "1. PCA\n",
    "1. model built by grid search\n",
    "\n",
    "## Tricks:\n",
    "Since the dataset is very imbalanced:\n",
    "1. using \"class_weight='balanced'\" to adjust weights of observations, and\n",
    "2. using F1 score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) LR pipeline\n",
    "hyper-parameter:\n",
    "1. C: np.logspace(-6,6,100)\n",
    "2. penalty: \"none\", \"l1\", \"l2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) LR with penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'C': 0.07054802310718646, 'penalty': 'l2'}\n",
      "Training F1: 0.1792, accuracy: 0.7286\n",
      "Testing F1: 0.1800, accuracy: 0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    5.6s finished\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = make_pipeline(\n",
    "    PCA(n_components=3),\n",
    "    GridSearchCV( # grid search for hyper-parameters\n",
    "        LogisticRegression(\n",
    "            solver='liblinear',\n",
    "            class_weight='balanced',\n",
    "            random_state=1,\n",
    "        ), \n",
    "        param_grid={\n",
    "            'C': np.logspace(-6,6,100),\n",
    "            \"penalty\": [\"l1\", \"l2\"]\n",
    "        }, \n",
    "        cv=5, # 5 fold Cross-Validation\n",
    "        scoring='f1',\n",
    "        verbose=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe_lr.fit(X_train_std, y_train)\n",
    "param_best_lr = pipe_lr.steps[1][1].best_params_\n",
    "y_train_pred = pipe_lr.predict(X_train_std)\n",
    "y_test_pred = pipe_lr.predict(X_test_std)\n",
    "f1_train_lr = f1_score(y_train, y_train_pred)\n",
    "f1_test_lr = f1_score(y_test, y_test_pred)\n",
    "acc_train_lr = accuracy_score(y_train, y_train_pred)\n",
    "acc_test_lr = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best hyper-parameters: {}\\nTraining F1: {:.4f}, accuracy: {:.4f}\\nTesting F1: {:.4f}, accuracy: {:.4f}\".format(\n",
    "    param_best_lr,\n",
    "    f1_train_lr,\n",
    "    acc_train_lr,\n",
    "    f1_test_lr,\n",
    "    acc_test_lr\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) LR with no penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1: 0.1798, accuracy: 0.7298\n",
      "Testing F1: 0.1800, accuracy: 0.7270\n"
     ]
    }
   ],
   "source": [
    "pipe_lr2 = make_pipeline(\n",
    "    PCA(n_components=3),\n",
    "    LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=1,\n",
    "    ), \n",
    ")\n",
    "\n",
    "pipe_lr2.fit(X_train_std, y_train)\n",
    "y_train_pred = pipe_lr2.predict(X_train_std)\n",
    "y_test_pred = pipe_lr2.predict(X_test_std)\n",
    "f1_train_lr2 = f1_score(y_train, y_train_pred)\n",
    "f1_test_lr2 = f1_score(y_test, y_test_pred)\n",
    "acc_train_lr2 = accuracy_score(y_train, y_train_pred)\n",
    "acc_test_lr2 = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Training F1: {:.4f}, accuracy: {:.4f}\\nTesting F1: {:.4f}, accuracy: {:.4f}\".format(\n",
    "    f1_train_lr2,\n",
    "    acc_train_lr2,\n",
    "    f1_test_lr2,\n",
    "    acc_test_lr2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion: LR with no penalty is OK.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) SVM pipeline\n",
    "hyper-parameter:\n",
    "1. C: np.logspace(-2,2,4)\n",
    "2. gamma: np.logspace(-2,2,4) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'C': 0.21544346900318834, 'gamma': 0.005}\n",
      "Training F1: 0.1860, accuracy: 0.7345\n",
      "Testing F1: 0.1866, accuracy: 0.7359\n"
     ]
    }
   ],
   "source": [
    "pipe_svm = make_pipeline(\n",
    "    PCA(n_components=3),\n",
    "    GridSearchCV(\n",
    "        SVC(\n",
    "            kernel='rbf', \n",
    "            tol=1e-2,\n",
    "            class_weight='balanced',\n",
    "            random_state=1,\n",
    "        ), \n",
    "        param_grid={\n",
    "            'C': np.logspace(-2,2,4),\n",
    "            'gamma': np.logspace(-2,2,4) * 0.5,\n",
    "        }, \n",
    "        cv=5, \n",
    "        scoring='f1',\n",
    "        verbose=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe_svm.fit(X_train_std, y_train)\n",
    "param_best_svm = pipe_svm.steps[1][1].best_params_\n",
    "y_train_pred = pipe_svm.predict(X_train_std)\n",
    "y_test_pred = pipe_svm.predict(X_test_std)\n",
    "f1_train_svm = f1_score(y_train, y_train_pred)\n",
    "f1_test_svm = f1_score(y_test, y_test_pred)\n",
    "acc_train_svm = accuracy_score(y_train, y_train_pred)\n",
    "acc_test_svm = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best hyper-parameters: {}\\nTraining F1: {:.4f}, accuracy: {:.4f}\\nTesting F1: {:.4f}, accuracy: {:.4f}\".format(\n",
    "    param_best_svm,\n",
    "    f1_train_svm,\n",
    "    acc_train_svm,\n",
    "    f1_test_svm,\n",
    "    acc_test_svm\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Decision Tree pipeline\n",
    "hyper-parameter:\n",
    "1. criterion: [\"gini\", \"entropy\"]\n",
    "2. max_depth: range(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters: {'criterion': 'gini', 'max_depth': 3}\n",
      "Training F1: 0.2015, accuracy: 0.7213\n",
      "Testing F1: 0.1873, accuracy: 0.7223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:    1.2s finished\n"
     ]
    }
   ],
   "source": [
    "pipe_tree = make_pipeline(\n",
    "    PCA(n_components=3),\n",
    "    GridSearchCV(\n",
    "        DecisionTreeClassifier(\n",
    "            class_weight='balanced',\n",
    "            random_state=1,\n",
    "        ), \n",
    "        param_grid={\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": range(1, 10)\n",
    "        }, \n",
    "        cv=5, \n",
    "        scoring='f1',\n",
    "        verbose=1\n",
    "    )\n",
    ")\n",
    "\n",
    "pipe_tree.fit(X_train_std, y_train)\n",
    "param_best_tree = pipe_tree.steps[1][1].best_params_\n",
    "y_train_pred = pipe_tree.predict(X_train_std)\n",
    "y_test_pred = pipe_tree.predict(X_test_std)\n",
    "f1_train_tree = f1_score(y_train, y_train_pred)\n",
    "f1_test_tree = f1_score(y_test, y_test_pred)\n",
    "acc_train_tree = accuracy_score(y_train, y_train_pred)\n",
    "acc_test_tree = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Best hyper-parameters: {}\\nTraining F1: {:.4f}, accuracy: {:.4f}\\nTesting F1: {:.4f}, accuracy: {:.4f}\".format(\n",
    "    param_best_tree,\n",
    "    f1_train_tree,\n",
    "    acc_train_tree,\n",
    "    f1_test_tree,\n",
    "    acc_test_tree\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional\n",
    "prove F1 is better metrics in this imbalanced situation comparing with Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1: 1.0000, accuracy: 1.0000\n",
      "Testing F1: 0.0929, accuracy: 0.9003\n"
     ]
    }
   ],
   "source": [
    "pipe_tree2 = make_pipeline(\n",
    "    PCA(n_components=3),\n",
    "    DecisionTreeClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=1,\n",
    "    ), \n",
    ")\n",
    "\n",
    "pipe_tree2.fit(X_train_std, y_train)\n",
    "\n",
    "y_train_pred = pipe_tree2.predict(X_train_std)\n",
    "y_test_pred = pipe_tree2.predict(X_test_std)\n",
    "f1_train_tree2 = f1_score(y_train, y_train_pred)\n",
    "f1_test_tree2 = f1_score(y_test, y_test_pred)\n",
    "acc_train_tree2 = accuracy_score(y_train, y_train_pred)\n",
    "acc_test_tree2 = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training F1: {:.4f}, accuracy: {:.4f}\\nTesting F1: {:.4f}, accuracy: {:.4f}\".format(\n",
    "    f1_train_tree2,\n",
    "    acc_train_tree2,\n",
    "    f1_test_tree2,\n",
    "    acc_test_tree2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy scores are very high, the F1 score of testing set is extremely low.\n",
    "Let's see the confusion metrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# tree trained under F1 metric:\n",
      "Testing F1: 0.1873, accuracy: 0.7223\n",
      "Confusion_matrix: \n",
      "[[2028  755]\n",
      " [  61   94]]\n",
      "Precision: 0.6065\n",
      "Recall: 0.1107\n",
      "\n",
      "# tree trained under accuracy metric:\n",
      "Testing F1: 0.0929, accuracy: 0.9003\n",
      "Confusion_matrix: \n",
      "[[2630  153]\n",
      " [ 140   15]]\n",
      "Precision: 0.0968\n",
      "Recall: 0.0893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_tree = pipe_tree.predict(X_test_std)\n",
    "y_test_pred_tree2 = pipe_tree2.predict(X_test_std)\n",
    "confusion_mtx = confusion_matrix(y_test, y_test_pred_tree)\n",
    "confusion_mtx2 = confusion_matrix(y_test, y_test_pred_tree2)\n",
    "prec_ratio = confusion_mtx[-1, -1] / np.sum(confusion_mtx[-1, :])\n",
    "prec_ratio2 = confusion_mtx2[-1, -1] / np.sum(confusion_mtx2[-1, :])\n",
    "recall_ratio = confusion_mtx[-1, -1] / np.sum(confusion_mtx[:, -1])\n",
    "recall_ratio2 = confusion_mtx2[-1, -1] / np.sum(confusion_mtx2[:, -1])\n",
    "\n",
    "print(\"\"\"\n",
    "# tree trained under F1 metric:\n",
    "Testing F1: {:.4f}, accuracy: {:.4f}\n",
    "Confusion_matrix: \n",
    "{}\n",
    "Precision: {:.4f}\n",
    "Recall: {:.4f}\n",
    "\n",
    "# tree trained under accuracy metric:\n",
    "Testing F1: {:.4f}, accuracy: {:.4f}\n",
    "Confusion_matrix: \n",
    "{}\n",
    "Precision: {:.4f}\n",
    "Recall: {:.4f}\n",
    "\"\"\".format(\n",
    "    f1_test_tree,\n",
    "    acc_test_tree,\n",
    "    confusion_mtx,\n",
    "    prec_ratio,\n",
    "    recall_ratio,\n",
    "    f1_test_tree2,\n",
    "    acc_test_tree2,\n",
    "    confusion_mtx2,\n",
    "    prec_ratio2,\n",
    "    recall_ratio2,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the precision ratio of tree trained under accuracy metric is significantly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
